"""Pipeline endpoints — trigger scout, generate, regenerate, and full runs."""

import datetime

from fastapi import APIRouter, Depends
from pydantic import BaseModel

from database import get_data_layer
from models import ContentChannel
from services.data_layer import DataLayer
from services.scout import run_full_scout, filter_signals_for_relevance
from services.engine import generate_brief, generate_all_content, regenerate_single
from services.humanizer import humanize

router = APIRouter(prefix="/api/pipeline", tags=["pipeline"])


def _build_company_context(voice: dict) -> str:
    """Build a compact company description for the relevance filter."""
    parts = []
    name = voice.get("onboard_company_name", "")
    if name:
        parts.append(f"Company: {name}")
    industry = voice.get("onboard_industry", "")
    if industry:
        parts.append(f"Industry: {industry}")
    topics = voice.get("onboard_topics", "")
    if topics:
        parts.append(f"Topics: {topics}")
    competitors = voice.get("onboard_competitors", "")
    if competitors:
        parts.append(f"Competitors: {competitors}")
    audience = voice.get("voice_audience", "")
    if audience:
        parts.append(f"Audience: {audience}")
    persona = voice.get("voice_persona", "")
    if persona:
        parts.append(f"Persona: {persona}")
    return "\n".join(parts) if parts else "General technology company"


@router.post("/scout")
async def trigger_scout(since_hours: int = 24, dl: DataLayer = Depends(get_data_layer)):
    """Run the scout — pull signals from all sources."""
    api_key = await dl.resolve_api_key()
    org_settings = await dl.get_all_settings()
    raw_signals = await run_full_scout(since_hours, org_settings=org_settings)

    # Relevance filter — discard off-topic noise
    voice = await dl.get_voice_settings()
    company_ctx = _build_company_context(voice)
    signals = await filter_signals_for_relevance(raw_signals, company_ctx, api_key=api_key)

    # Prune signals older than 7 days
    pruned = await dl.prune_old_signals(days=7)

    # Save with URL dedup — skip signals we already have
    saved = []
    skipped = 0
    for s in signals:
        url = s.get("url", "")
        if url and await dl.signal_exists(url):
            skipped += 1
            continue
        result = await dl.save_signal(s)
        saved.append(result)

    await dl.commit()
    return {
        "signals_raw": len(raw_signals),
        "signals_relevant": len(signals),
        "signals_saved": len(saved),
        "signals_skipped_dupes": skipped,
        "signals_pruned": pruned,
        "signals": [{"title": s.get("title", ""), "type": s.get("type", ""), "source": s.get("source", "")} for s in saved],
    }


@router.post("/generate")
async def trigger_generate(
    channels: list[str] | None = None,
    dl: DataLayer = Depends(get_data_layer),
):
    """Generate content from today's signals. Runs brief → content → humanizer."""
    signal_dicts = await dl.list_signals(limit=20)

    if not signal_dicts:
        return {"error": "No signals found. Run /api/pipeline/scout first."}

    # Load voice settings and memory context
    voice = await dl.get_voice_settings()
    memory = await dl.get_memory_context()

    # Resolve API key for this org
    api_key = await dl.resolve_api_key()

    # Generate structured brief with per-channel angles
    brief_data = await generate_brief(signal_dicts, memory=memory, voice_settings=voice, api_key=api_key)
    brief = await dl.save_brief({
        "date": str(datetime.date.today()),
        "summary": brief_data["summary"],
        "angle": brief_data["angle"],
        "signal_ids": ",".join(str(s.get("id", "")) for s in signal_dicts[:10]),
    })

    # Parse channels
    target_channels = None
    if channels:
        target_channels = [ContentChannel(c) for c in channels]

    # Load assets for system prompt context
    assets = await dl.list_assets()

    # Generate content — each channel gets its own signal selection and angle
    content_items = await generate_all_content(
        brief_data, signal_dicts, target_channels,
        memory=memory, voice_settings=voice, assets=assets,
        api_key=api_key,
    )

    saved_content = []
    for item in content_items:
        raw_body = item["body"]
        clean_body = humanize(raw_body)
        result = await dl.save_content({
            "brief_id": brief.get("id"),
            "signal_id": signal_dicts[0].get("id") if signal_dicts else None,
            "channel": item["channel"],
            "status": "queued",
            "headline": item["headline"],
            "body": clean_body,
            "body_raw": raw_body,
            "author": "company",
            "source_signal_ids": item.get("source_signal_ids", ""),
        })
        saved_content.append(result)
        # Increment usage count on each source signal
        for sid in (item.get("source_signal_ids", "") or "").split(","):
            sid = sid.strip()
            if sid and sid.isdigit():
                await dl.increment_signal_usage(int(sid))

    await dl.commit()
    return {
        "brief": {"id": brief.get("id"), "angle": brief_data["angle"]},
        "content_generated": len(saved_content),
        "items": [{"id": c.get("id"), "channel": c.get("channel", ""), "headline": c.get("headline", "")} for c in saved_content],
    }


class RegenerateRequest(BaseModel):
    feedback: str = ""


@router.post("/regenerate/{content_id}")
async def regenerate_content(content_id: int, req: RegenerateRequest,
                              dl: DataLayer = Depends(get_data_layer)):
    """Regenerate a single piece of content with optional editor feedback."""
    existing = await dl.get_content(content_id)
    if not existing:
        return {"error": "Content not found"}

    channel = ContentChannel(existing["channel"])
    voice = await dl.get_voice_settings()
    memory = await dl.get_memory_context()
    api_key = await dl.resolve_api_key()

    # Use the raw (pre-humanizer) body as source, fall back to cleaned body
    source_body = existing.get("body_raw", "") or existing.get("body", "")

    result = await regenerate_single(
        source_body, channel,
        feedback=req.feedback,
        memory=memory, voice_settings=voice,
        api_key=api_key,
    )

    # Humanize and update the content record
    raw_body = result["body"]
    clean_body = humanize(raw_body)

    await dl.update_content_status(content_id, "queued",
                                    headline=result["headline"],
                                    body=clean_body,
                                    body_raw=raw_body)
    await dl.commit()

    return {
        "id": content_id,
        "channel": channel.value,
        "headline": result["headline"],
        "status": "queued",
    }


@router.post("/run")
async def full_run(since_hours: int = 24, dl: DataLayer = Depends(get_data_layer)):
    """Full pipeline: scout → brief → generate → humanize → queue."""
    api_key = await dl.resolve_api_key()
    org_settings = await dl.get_all_settings()
    raw_signals = await run_full_scout(since_hours, org_settings=org_settings)

    # Relevance filter — discard off-topic noise before generating content
    voice = await dl.get_voice_settings()
    company_ctx = _build_company_context(voice)
    filtered_signals = await filter_signals_for_relevance(raw_signals, company_ctx, api_key=api_key)

    # Prune old signals + dedup
    await dl.prune_old_signals(days=7)

    saved_signals = []
    for s in filtered_signals:
        url = s.get("url", "")
        if url and await dl.signal_exists(url):
            continue
        result = await dl.save_signal(s)
        saved_signals.append(result)

    if not saved_signals:
        await dl.commit()
        return {"status": "no_signals", "message": "Scout found nothing. Wire is quiet."}

    signal_dicts = saved_signals

    # Load voice settings and memory context
    voice = await dl.get_voice_settings()
    memory = await dl.get_memory_context()

    # Structured brief with per-channel angles
    brief_data = await generate_brief(signal_dicts, memory=memory, voice_settings=voice, api_key=api_key)
    brief = await dl.save_brief({
        "date": str(datetime.date.today()),
        "summary": brief_data["summary"],
        "angle": brief_data["angle"],
        "signal_ids": ",".join(str(s.get("id", "")) for s in signal_dicts[:10]),
    })

    # Load assets for system prompt context
    assets = await dl.list_assets()

    # Generate all channels — each gets targeted signals and its own angle
    content_items = await generate_all_content(
        brief_data, signal_dicts,
        memory=memory, voice_settings=voice, assets=assets,
        api_key=api_key,
    )

    saved_content = []
    for item in content_items:
        raw_body = item["body"]
        clean_body = humanize(raw_body)
        result = await dl.save_content({
            "brief_id": brief.get("id"),
            "signal_id": signal_dicts[0].get("id") if signal_dicts else None,
            "channel": item["channel"],
            "status": "queued",
            "headline": item["headline"],
            "body": clean_body,
            "body_raw": raw_body,
            "author": "company",
            "source_signal_ids": item.get("source_signal_ids", ""),
        })
        saved_content.append(result)
        # Increment usage count on each source signal
        for sid in (item.get("source_signal_ids", "") or "").split(","):
            sid = sid.strip()
            if sid and sid.isdigit():
                await dl.increment_signal_usage(int(sid))

    await dl.commit()

    return {
        "status": "complete",
        "signals": len(saved_signals),
        "brief": {"id": brief.get("id"), "angle": brief_data["angle"]},
        "content": [{"id": c.get("id"), "channel": c.get("channel", ""), "headline": c.get("headline", "")} for c in saved_content],
    }
